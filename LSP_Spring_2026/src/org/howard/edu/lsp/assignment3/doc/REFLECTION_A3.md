Assignment 2 and Assignment 3 solve the same ETL problem, but they have very different designs. In Assignment 2, my program was essentially a single procedural pipeline wrapped inside one Java class. The main method basically controlled everything: it opened the input and output files, skipped the header, read each row, enforced the row-skipping rules, transformed the data, wrote the output line, tracked counters, and printed the run summary. Although I did have a helper method named transform, it still took on multiple responsibilities. It parsed numeric values, applied the given transformations, and returned a formatted CSV output row to main. As a result, the program behaved correctly, but the structure was very far removed from objected oriented design-patterns, especially since the main method functioned as a god method.
In Assignment 3, I redesigned the program using object-oriented decomposition while keeping the same behavior as in Assignment 2. The most important design difference is that Assignment 3 separates the ETL pipeline into many classes that have clearer responsibilities. Instead of having one class do everything, the coordinating logic stays in ETLPipeline, and the other steps are focused on their own components. Reading is handled by CsvProductReader, which hides the details of BufferedReader usage and ensures the header is skipped exactly once, matching the conditions imposed by Assignment 2. The writing portion of the program is handled by CsvProductWriter, which concentrates on output creation and guarantees that the header row is written first. Parsing and validation are handled by ProductRowParser, which encapsulates the row-skipping rules (blank lines, incorrect field count, and parsing failures) and trims whitespace as Assignment 2 requires. The transformations themselves are isolated in DefaultProductTransformer. Finally, RunSummary prints the summary in the same way as Assignment 2. Using this decomposition, the program is clearer in what it is meant to do and allows us to follow certain object-oriented heuristics. Each responsibility that the program tackles is stored in a class, and ETLPipeline connects each piece together.
Moreover, Assignment 3 is much more object-oriented because it models the program as a set of interacting objects rather than a single block procedure. One major improvement is that the data itself is represented with domain objects. ProductInputRecord encapsulates a validated input row (typed product ID and price, plus name and category), and ProductOutputRecord encapsulates a transformed row in the exact output schema. In Assignment 2, the “record” was basically an array of strings and later a single output string; in Assignment 3, the record becomes an object with fields, which makes the code more explicit and less error-prone. This also improves encapsulation, because the formatting rules for the output CSV line are contained inside ProductOutputRecord.toCsvRow(). The rest of the program does not need to know how the output row is constructed; it simply asks the object to format itself. Similarly, the parsing and validation logic is hidden inside ProductRowParser, so the ETL pipeline does not need to manage trimming, splitting, or numeric parsing directly.
In terms of object-oriented ideas, Assignment 3 uses classes and objects throughout to represent responsibilities and data structures. Encapsulation shows up through hiding the internal details of each class. For example, the pipeline does not interact with the file directly; it requests the data found from CsvProductReader. Polymorphism is demonstrated through the ProductTransformer interface. The pipeline depends on the transformer contract rather than a specific implementation, and DefaultProductTransformer implements that contract. This is a concrete example of “programming to an interface,” since ETLPipeline can work with any transformer that implements ProductTransformer without changing the pipeline loop. However, because there was no inherent class hierarchy, I did not implement inheritance.
Lastly, I tested the program using the same cases described in the Assignment 2 to confirm that it functions correctly. I used the very same sample given in Assignment 2. Furthermore, I verified that the output file was the same as the one I generated in Assignment 2. As for the error handling, I ensured that given an empty file (header only), we would still generate a file with only a header. Lastly, I tested the missing input file case to certify that an error message would be printed and the program would be exited cleanly. With these tests, I am confident that we have successfully changed the design of the program, but not its functional behavior.
